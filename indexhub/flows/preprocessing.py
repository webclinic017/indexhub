import io
import logging
import os
from datetime import date
from typing import List, Mapping

import boto3
import polars as pl
from botocore.exceptions import ClientError
from prefect import flow, task
from xlsx2csv import XlsxValueError


def determine_dt_format(dt: str):
    if len(dt) == 19:
        format = "%Y-%m-%d %H:%M:%S"
    elif len(dt) == 10:
        format = "%Y-%m-%d"
    elif len(dt) == 7:
        if "-" in dt:
            format = "%Y-%m"
    elif len(dt) == 6:
        format = "%Y%m"
    else:
        raise Exception(
            "Datetime format for time col is not acceptable. Please review the datetime column"
        )

    return format


def setup_aws():
    # Retrieve the AWS credentials from environment variables
    AWS_ACCESS_KEY_ID = os.environ["AWS_ACCESS_KEY_ID"]
    AWS_SECRET_ACCESS_KEY = os.environ["AWS_SECRET_ACCESS_KEY"]
    AWS_SESSION_TOKEN = os.environ["AWS_SESSION_TOKEN"]

    # Set up the boto3 client to access S3 using the environment variable
    s3_client = boto3.resource(
        "s3",
        aws_access_key_id=AWS_ACCESS_KEY_ID,
        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
        aws_session_token=AWS_SESSION_TOKEN,
    )

    return s3_client


def check_filename(
    s3_client,
    s3_bucket: str,
    s3_dir: str,
    s3_filename: str = None,
):
    try:
        s3_client.Object(s3_bucket, f"{s3_dir}{s3_filename}").load()
    except ClientError as err:
        if err.response["Error"]["Code"] == "404":
            logging.warning(f"The forecast file {s3_filename} does not exist in s3")
            return False
        else:
            logging.error(err)
            raise
    else:
        logging.info(f"The forecast file {s3_filename} exists in s3")
        return True


@task
def load_raw_data(
    s3_client,
    s3_bucket: str,
    s3_dir: str,
    s3_filename: str,
):

    try:
        # io.BytesIO might be giving issue > changing blanks to "#N/A"
        s3object = s3_client.Object(s3_bucket, f"{s3_dir}{s3_filename}")
        obj = s3object.get()["Body"].read()

        raw_df = pl.read_excel(
            io.BytesIO(obj),
            # Ignore infer datatype to float as it is not supported by xlsx2csv
            xlsx2csv_options={"ignore_formats": "float"},
            read_csv_options={"infer_schema_length": None},
        )

    except (ClientError, ValueError, XlsxValueError) as exc:
        raise exc

    return raw_df


@task
def load_econdb_data(selected_series_to_name: Mapping[str, str]):
    path = "https://www.econdb.com/api/series/"

    econdb_data = []
    for series_name in selected_series_to_name.keys():
        try:
            url = f"{path}{series_name}/?format=csv"
            df = pl.read_csv(url, parse_dates=True)
            econdb_data.append(df)
        except TypeError as exc:
            raise exc

    return econdb_data


@task
def clean_raw_data(
    df: pl.DataFrame,
    time_col: str,
    target_col: str,
    index_cols: List[str],
    suffix: str,
):
    # Infer datetime format by using the first value
    fmt = determine_dt_format(str(df.select([time_col])[0, 0]))

    cleaned_df = df.rename(
        {target_col: f"{target_col}:{suffix}", time_col: "time"}
    ).with_columns(
        [
            # Parse datatime
            (pl.col("time").cast(pl.Utf8).str.strptime(pl.Date, fmt=fmt)),
            # Cast all float64 cols to int64
            pl.col(pl.Float64).cast(pl.Int64),
            # Defensive replace #N/A generated by nulls/blanks to 0
            pl.col(index_cols).str.replace("#N/A", "0"),
        ]
    )

    return cleaned_df


@task
def clean_econdb_data(
    data: List[pl.DataFrame], series_to_name_mapping: Mapping[str, str]
):
    series_names = list(series_to_name_mapping)
    new_colnames = list(series_to_name_mapping.values())
    stg_external_data = []
    time_col = "Date"

    for i, df in enumerate(data):
        # Identify the length of the first value (time_col)
        fmt = determine_dt_format(str(df.select([time_col])[0, 0]))

        wrangled_df = (
            # Rename series name to appropriate colnames
            df.select([time_col, series_names[i]])
            .rename({series_names[i]: new_colnames[i], time_col: "time"})
            .with_columns(
                [
                    # Parse datetime
                    pl.col("time").cast(pl.Utf8).str.strptime(pl.Date, fmt=fmt),
                    # Cast the column type to float64
                    pl.col(new_colnames[i]).cast(pl.Float64),
                ]
            )
        )
        stg_external_data.append(wrangled_df)

    return stg_external_data


@task
def filter_data(
    df: pl.DataFrame,
    filter_col: str,
    exclude_values: str,
    target_col: str,
    suffix: str,
):
    filtered_df = df.filter(
        ~pl.col(filter_col).is_in(exclude_values)
        & (pl.col(f"{target_col}:{suffix}") >= 0)
    )

    return filtered_df


@task
def concat_data(data: List[pl.DataFrame]):
    joined_data = pl.concat(data, how="diagonal").pipe(
        lambda df: df.with_columns(
            [pl.col(col).fill_null(0) for col in list(df.columns) if col != "time"]
        )
    )

    return joined_data


@task
def join_econdb_stg_data(data: List[pl.DataFrame]):
    int_external_data = (
        data[0]
        .join(data[1], on="time")
        .join(data[2], on="time")
        .pipe(
            lambda df: df.with_columns(
                [pl.col(col).fill_null(0) for col in list(df.columns) if col != "time"]
            )
        )
    )

    return int_external_data


@task
def add_calendar_effects(df: pl.DataFrame):
    assigned_df = df.with_columns(
        [
            (pl.col("time").dt.month()).alias("month").cast(pl.Int64),
            (pl.col("time").dt.year()).alias("year").cast(pl.Int64),
            (pl.col("time").dt.month())
            .is_in([1, 2])
            .alias("is_holiday")
            .cast(pl.Int64),
        ]
    )

    return assigned_df


@task
def add_entity_effects(df: pl.DataFrame, country_cols: Mapping[str, str]):

    for country, country_col in country_cols.items():
        df = df.with_column(
            # change to .contains (to lowercase) - make it case insensitive
            pl.col(country_col)
            .str.to_lowercase()
            .str.starts_with((country).lower())
            .alias(f"is_{country.lower()}")
            .cast(pl.Int64)
        )

    return df


@task
def aggregate_stg_sales_to_int(df: pl.DataFrame, index_cols: List[str]):
    agg_by_sum = [col for col in df.columns if col not in (["time"] + index_cols)]
    # Aggregate and upsample with ffil and fill_null
    agg_data = (
        df.groupby(index_cols + ["time"])
        .agg([pl.sum(col) for col in agg_by_sum])
        .sort("time")
        .upsample("time", every="1mo", by=index_cols)
        .with_column(pl.col(index_cols).forward_fill())
        .fill_null(0)
    )

    return agg_data


@task
def join_int_data(int_sales_data: pl.DataFrame, int_external_data: pl.DataFrame):
    ftr_sales_data = (
        int_sales_data.join(int_external_data, on="time", how="left")
        .fill_null(0)
        .sort("time")
    )

    return ftr_sales_data


@task
def export_ftr_table_to_parquet(
    s3_bucket: str, df: pl.DataFrame, s3_output_dir: str, source_name: str
):
    s3_path = f"s3://{s3_bucket}/{s3_output_dir}/ftr_{source_name}_data"
    updated_date = date.today().strftime("%Y%m%d")
    # Use pandas to_parquet to export data to s3
    try:
        df.to_pandas().to_parquet(f"{s3_path}_{updated_date}.parquet", index=False)

    except PermissionError as err:
        logging.warning(err)
        raise err


@task
def map_ftr_table_ids_to_paths(
    table_ids_to_dataframes: Mapping[str, str], s3_bucket: str, s3_output_dir: str
):
    updated_date = date.today().strftime("%Y%m%d")
    table_to_path = {}
    for table_id in table_ids_to_dataframes.keys():
        if "ftr" in table_id:
            # Store the ftr table based on the source/function in the name
            s3_path = f"s3://{s3_bucket}/{s3_output_dir}/{table_id}"
            table_to_path[table_id] = f"{s3_path}_{updated_date}.parquet"

    return table_to_path


@flow()
def preprocess_data(
    s3_client,
    s3_bucket: str,
    s3_dir: str,
    s3_output_dir: str,
    time_col: str,
    target_col: str,
    index_cols: List[str],
    entity_groups: Mapping[str, str],
    s3_sales_data_filename: str,
    s3_forecast_data_filename: str = None,
    filter_col: str = None,
    exclude_values: List[str] = None,
):

    # Parameters for econdb
    _selected_econdb_series_to_name = {
        "NBS_PRICE.IP_2010.M.CN": "industrial_production",
        "OILPRODCN": "oil_production",
        "NBS_A010301M.A01030105.M.CN": "cpi",
    }
    # Initial state
    raw_forecast_df = None
    stg_sales_forecast = None

    # Load dataset from client
    raw_sales_df = load_raw_data(s3_client, s3_bucket, s3_dir, s3_sales_data_filename)

    # Clean raw_sales_df
    stg_sales_actual = clean_raw_data(
        df=raw_sales_df,
        time_col=time_col,
        target_col=target_col,
        index_cols=index_cols,
        suffix="actual",
    )

    # Check if the forecast file exists in the S3 bucket
    if check_filename(s3_client, s3_bucket, s3_dir, s3_forecast_data_filename) is True:
        raw_forecast_df = load_raw_data(
            s3_client, s3_bucket, s3_dir, s3_forecast_data_filename
        )

        stg_sales_forecast = clean_raw_data(
            df=raw_forecast_df,
            time_col=time_col,
            target_col=target_col,
            index_cols=index_cols,
            suffix="forecast",
        )

    # Filter stg_sales data
    if filter_col is not None:
        stg_sales_actual = filter_data(
            df=stg_sales_actual,
            filter_col=filter_col,
            exclude_values=exclude_values,
            target_col=target_col,
            suffix="actual",
        )

        if stg_sales_forecast is not None:
            stg_sales_forecast = filter_data(
                df=stg_sales_forecast,
                filter_col=filter_col,
                exclude_values=exclude_values,
                target_col=target_col,
                suffix="forecast",
            )

    # Concat stg_sales data if there is stg_sales_forecast
    if stg_sales_forecast is not None:
        int_sales_data = (
            concat_data([stg_sales_actual, stg_sales_forecast])
            .pipe(lambda df: aggregate_stg_sales_to_int(df, index_cols))
            .pipe(lambda df: add_calendar_effects(df))
            .pipe(lambda df: add_entity_effects(df, entity_groups))
        )
    else:
        int_sales_data = (
            aggregate_stg_sales_to_int(stg_sales_actual, index_cols)
            .pipe(lambda df: add_calendar_effects(df))
            .pipe(lambda df: add_entity_effects(df, entity_groups))
        )

    # Load external datasets
    raw_external_data = load_econdb_data(
        selected_series_to_name=_selected_econdb_series_to_name
    )

    # Clean raw_external_data
    stg_external_data = clean_econdb_data(
        data=raw_external_data, series_to_name_mapping=_selected_econdb_series_to_name
    )

    # Join all econdb external dfs
    int_external_data = join_econdb_stg_data(stg_external_data)

    # Join internal and external data
    ftr_sales_data = join_int_data(int_sales_data, int_external_data)

    # Get the source id for table name
    source_id = s3_output_dir.split("/")[-1]

    # Mapping of all table_ids with their corresponding dataframes
    table_ids_to_dataframes = {
        "stg_external_industrial_production_index_china": stg_external_data[0],
        "stg_external_oil_production_china": stg_external_data[1],
        "stg_external_consumer_price_index_china": stg_external_data[2],
        f"stg_{source_id}_actual": stg_sales_actual,
        f"stg_{source_id}_forecast": stg_sales_forecast,
        f"int_{source_id}_data": int_sales_data,
        "int_external_data": int_external_data,
        f"ftr_{source_id}_data": ftr_sales_data,
    }

    # Export tables to parquet
    export_ftr_table_to_parquet(s3_bucket, ftr_sales_data, s3_output_dir, source_id)

    # Generate a dictionary mapping of ftr table_ids to their corresponding paths in s3
    ftr_table_ids_to_paths = map_ftr_table_ids_to_paths(
        table_ids_to_dataframes, s3_bucket, s3_output_dir
    )

    return ftr_table_ids_to_paths
