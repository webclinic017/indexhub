import io
import itertools
import re
from datetime import datetime
from hashlib import md5
from typing import List, Mapping, Optional

import boto3
import polars as pl
from prefect import flow, task
from typing_extensions import Literal

from indexhub.flows.forecasting import (
    PL_FREQ_TO_PD_FREQ,
    TEST_SIZE,
    get_train_test_splits,
    run_backtest,
    run_forecast,
)

MIN_TRAIN_SIZE = {"Q": 9, "MS": 12, "W": 18, "D": 30}


def infer_dt_format(dt: str):
    n_chars = len(dt)
    if n_chars == 19:
        fmt = "%Y-%m-%d %H:%M:%S"
    elif n_chars == 10:
        fmt = "%Y-%m-%d"
    elif n_chars == 7:
        if "-" in dt:
            fmt = "%Y-%m"
    elif n_chars == 6:
        fmt = "%Y%m"
    else:
        raise ValueError(f"Failed to parse datetime: {dt}")
    return fmt


@task
def load_raw_panel(s3_bucket: str, s3_path: str) -> pl.LazyFrame:
    # io.BytesIO fails to change blanks to "#N/A"
    s3_client = boto3.resource("s3")
    s3_object = s3_client.Object(s3_bucket, s3_path)
    obj = s3_object.get()["Body"].read()
    raw_panel = pl.read_excel(
        io.BytesIO(obj),
        # Ignore infer datatype to float as it is not supported by xlsx2csv
        xlsx2csv_options={"ignore_formats": "float"},
        read_csv_options={"infer_schema_length": None},
    )
    return raw_panel.lazy()


@task
def load_fct_panel(s3_bucket: str, s3_path: str):
    s3_client = boto3.resource("s3")
    s3_object = s3_client.Object(s3_bucket, s3_path)
    obj = s3_object.get()["Body"].read()
    fct_panel = pl.read_parquet(io.BytesIO(obj))
    return fct_panel


@task
def select_rows(df: pl.LazyFrame, filters: Mapping[str, List[str]]):
    for col, values in filters.items():
        filtered_df = df.filter(~pl.col(col).is_in(values))
    return filtered_df.collect(streaming=True).lazy()


@task
def clean_raw_panel(
    df: pl.LazyFrame,
    time_col: str,
    entity_cols: List[str],
) -> pl.LazyFrame:

    # Infer datetime format by using the first value
    fmt = infer_dt_format(str(df.collect().select([time_col])[0, 0]))
    new_col_names = {time_col: "time"}
    df_new = (
        df.rename(new_col_names)
        .with_columns(
            [
                # Parse datatime
                (pl.col("time").cast(pl.Utf8).str.strptime(pl.Date, fmt=fmt)),
                # Cast all float cols to int
                pl.col(pl.Float32).cast(pl.Int32),
                pl.col(pl.Float64).cast(pl.Int64),
                # Defensive replace #N/A generated by nulls/blanks to 0
                pl.col(entity_cols).str.replace("#N/A", "0"),
            ]
        )
        # Downcast numeric dtypes
        .select(pl.all().shrink_dtype())
        # Sort by entity and time
        .sort(by=[*entity_cols, "time"])
        .collect(streaming=True)
    )
    return df_new.lazy()


@task
def add_holiday_effects(df: pl.LazyFrame) -> pl.LazyFrame:
    exprs = [
        pl.col("time").dt.month().is_in([1, 2]).alias("is_holiday"),
    ]
    df_new = df.with_columns(exprs).collect(streaming=True)
    return df_new.lazy()


@task
def add_entity_effects(df: pl.LazyFrame, levels: List[str]) -> pl.LazyFrame:
    entity_id = ":".join(levels)
    fixed_effects = pl.get_dummies(df.collect().select(entity_id)).select(
        pl.all().cast(pl.Boolean).prefix("is_")
    )
    # Replace spaces and semicolons with underscores
    fixed_effects.columns = [
        re.sub(r"[\s:]", "_", str(col).lower()) for col in fixed_effects.columns
    ]
    df_new = pl.concat([df.collect(), fixed_effects], how="horizontal")
    return df_new.lazy()


@task
def export_fct_panel(
    df: pl.LazyFrame, s3_bucket: str, raw_data_path: str, suffix: Optional[str] = None
):
    # Use the first 7 characters of the hash of raw data path as ID
    identifer = md5(raw_data_path.encode("utf-8")).hexdigest()[:7]
    ts = int(datetime.now().timestamp())
    s3_path = f"processed/{identifer}/{ts}"
    if suffix:
        s3_path = f"{s3_path}_{suffix}"
    df.collect().to_pandas().to_parquet(
        f"s3://{s3_bucket}/{s3_path}.parquet", index=False
    )
    return f"{s3_path}.parquet"


@flow
def preprocess_panel(
    s3_bucket: str,
    time_col: str,
    entity_cols: List[str],
    freq: str,
    raw_data_path: str,
    report_id: str,
    manual_forecast_path: Optional[str] = None,
    filters: Optional[Mapping[str, List[str]]] = None,
):
    try:
        raw_panel = load_raw_panel(s3_bucket, raw_data_path)
        raw_panel = (
            select_rows(raw_panel, filters) if filters is not None else raw_panel
        )
        fct_panel = clean_raw_panel(
            df=raw_panel,
            time_col=time_col,
            entity_cols=entity_cols,
        )
        paths = {"actual": export_fct_panel(fct_panel, s3_bucket, raw_data_path)}

        if manual_forecast_path is not None:
            manual_forecasts = load_raw_panel(s3_bucket, manual_forecast_path)
            manual_forecasts = (
                select_rows(manual_forecasts, filters)
                if filters is not None
                else manual_forecasts
            )
            fct_manual_forecast = clean_raw_panel(
                df=manual_forecasts,
                time_col=time_col,
                entity_cols=entity_cols,
            )
            paths["manual"] = export_fct_panel(
                fct_manual_forecast, s3_bucket, raw_data_path, suffix="manual"
            )

        start_date = fct_panel.collect().select(pl.min("time"))[0, 0]
        end_date = fct_panel.collect().select(pl.max("time"))[0, 0]

        paths["metadata"] = {
            "report_id": report_id,
            "freq": freq,
            "start_date": start_date.strftime("%Y-%m-%d"),
            "end_date": end_date.strftime("%Y-%m-%d"),
            "status": "SUCCESS",
        }

    except Exception as exc:
        paths = {
            "metadata": {
                "report_id": report_id,
                "freq": freq,
                "status": "FAILED",
                "msg": str(repr(exc)),
            }
        }
    return paths


@task
def reindex_ftr_panel(df: pl.DataFrame, suffix: str) -> pl.LazyFrame:
    # Create new index
    entity_col, time_col, target_col = df.columns
    dtypes = df.dtypes[:2]
    entities = df.get_column(entity_col).unique()
    timestamps = df.get_column(time_col).unique()
    full_idx = pl.DataFrame(
        itertools.product(entities, timestamps), columns=df.columns[:2]
    )
    # Defensive cast dtypes to be consistent with df
    full_idx = full_idx.select(
        [pl.col(col).cast(dtypes[i]) for i, col in enumerate(full_idx.columns)]
    )
    # Outer join
    df_new = (
        df.join(full_idx, on=[entity_col, time_col], how="outer")
        # Fill nulls with 0 and rename target col (actual/manual/forecast)
        .with_column(
            pl.col(target_col).fill_null(pl.lit(0)).alias(f"target:{suffix}")
        ).drop(target_col)
    )
    return df_new.lazy()


@task
def groupby_aggregate(
    df: pl.DataFrame,
    levels: List[str],
    target_col: str,
    agg_by: str,
    freq: str,
) -> pl.DataFrame:
    entity_id = ":".join(levels)
    agg_methods = {
        "sum": pl.sum(target_col),
        "mean": pl.mean(target_col),
    }

    df_new = (
        # Assign new col with entity_id
        df.with_column(pl.concat_str(levels, sep=":").alias(entity_id))
        .sort("time")
        .groupby(["time", entity_id], maintain_order=True)
        .agg(agg_methods[agg_by])
        # Defensive reorder columns
        .select([entity_id, "time", target_col])
        # Defensive resampling
        .groupby_dynamic("time", every=freq, by=entity_id)
        .agg(agg_methods[agg_by])
    )
    return df_new


@task
def filter_negative_values(
    df: pl.DataFrame,
    target_col: str,
) -> pl.DataFrame:

    df_new = df.with_column(
        # Remove negative values from target col
        pl.when(pl.col(target_col) <= 0)
        .then(0)
        .otherwise(pl.col(target_col))
        .keep_name()
    )
    return df_new


@task
def coerce_entity_colname(df: pl.LazyFrame, levels: List[str]) -> pl.LazyFrame:
    entity_id = ":".join(levels)

    # Coerce entity column name and defensive sort columns
    df_new = df.select(
        [
            # Coerce entity column name
            pl.col(entity_id).alias("entity"),
            pl.col("time"),
            # Include target col with prefix "target"
            pl.col("^target_.*$"),
            # Drop original entity_id col
            pl.all().exclude(["time", entity_id]),
        ]
    )
    return df_new


@task
def export_ftr_panel(
    df: pl.LazyFrame, s3_bucket: str, fct_data_path: str, suffix: Optional[str] = None
):
    # Use hash of fct data path and entity col as ID
    dataset_id = fct_data_path + df.columns[0]
    identifer = md5(dataset_id.encode("utf-8")).hexdigest()[:7]
    ts = int(datetime.now().timestamp())
    s3_path = f"processed/{identifer}/{ts}"
    if suffix:
        s3_path = f"{s3_path}_{suffix}"
    df.collect().to_pandas().to_parquet(
        f"s3://{s3_bucket}/{s3_path}.parquet", index=False
    )
    return f"{s3_path}.parquet"


@flow
def prepare_hierarchical_panel(
    s3_bucket: str,
    levels: List[str],
    agg_method: Literal["sum", "mean"],
    fct_panel_path: str,
    target_col: str,
    date_features: List[str],
    freq: str,
    lags: List[int],
    manual_forecasts_path: Optional[str] = None,
    allow_negatives: Optional[bool] = False,
):
    fct_panel = load_fct_panel(s3_bucket, fct_panel_path)
    ftr_panel = (
        filter_negative_values(fct_panel, target_col)
        if allow_negatives is False
        else fct_panel
    )
    ftr_panel = (
        groupby_aggregate(ftr_panel, levels, target_col, agg_method, freq)
        .pipe(reindex_ftr_panel, suffix="actual")
        .pipe(add_holiday_effects)
        .pipe(add_entity_effects, levels)
        .pipe(coerce_entity_colname, levels)
    )
    paths = {"actual": export_ftr_panel(ftr_panel, s3_bucket, fct_panel_path)}

    if manual_forecasts_path is None:
        static_cols = [
            col
            for col in ftr_panel.select(pl.col(pl.Boolean)).columns
            if "is_holiday" not in col
        ]
        try:
            pd_freq = PL_FREQ_TO_PD_FREQ[freq]
        except KeyError as err:
            raise ValueError(f"Frequency `{freq}` is not supported.") from err
        fit_kwargs = {
            "static_cols": static_cols,
            "date_features": date_features,
            "freq": pd_freq,
            "lags": lags,
        }

        # Get length of time periods by entity
        y_len = (
            ftr_panel.groupby("entity").agg(pl.count()).collect().get_column("count")[0]
        )
        # Backtest period = length of time periods - minimum train size
        # n_splits = backtest period / test size
        n_splits = (y_len - MIN_TRAIN_SIZE[pd_freq]) // TEST_SIZE[pd_freq]

        n_split_to_ftr_train, n_split_to_ftr_test = get_train_test_splits(
            ftr_panel, n_splits, pd_freq
        )

        backtest, _ = run_backtest(
            n_split_to_ftr_train=n_split_to_ftr_train,
            n_split_to_ftr_test=n_split_to_ftr_test,
            fit_kwargs=fit_kwargs,
            quantile=None,
        )
        y_pred = run_forecast(ftr_panel=ftr_panel, fit_kwargs=fit_kwargs, quantile=None)
        ftr_manual_forecast = pl.concat([backtest, y_pred]).pipe(
            lambda df: df.rename({df.columns[-1]: "target:manual"})
        )
    else:
        fct_manual_forecast = load_fct_panel(s3_bucket, manual_forecasts_path)
        ftr_manual_forecast = (
            filter_negative_values(fct_manual_forecast, target_col)
            if allow_negatives is False
            else fct_manual_forecast
        )
        ftr_manual_forecast = (
            groupby_aggregate(ftr_manual_forecast, levels, target_col, agg_method, freq)
            .pipe(reindex_ftr_panel, suffix="manual")
            .pipe(coerce_entity_colname, levels)
        )
    paths["manual"] = export_ftr_panel(
        ftr_manual_forecast, s3_bucket, fct_panel_path, suffix="manual"
    )

    return paths
