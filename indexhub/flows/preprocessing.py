import io
import itertools
import re
from datetime import datetime
from hashlib import md5
from typing import List, Mapping, Optional

import boto3
import polars as pl
from prefect import flow, task
from typing_extensions import Literal


def infer_dt_format(dt: str):
    n_chars = len(dt)
    if n_chars == 19:
        fmt = "%Y-%m-%d %H:%M:%S"
    elif n_chars == 10:
        fmt = "%Y-%m-%d"
    elif n_chars == 7:
        if "-" in dt:
            fmt = "%Y-%m"
    elif n_chars == 6:
        fmt = "%Y%m"
    else:
        raise ValueError(f"Failed to parse datetime: {dt}")
    return fmt


@task
def load_raw_panel(s3_bucket: str, s3_path: str) -> pl.LazyFrame:
    # io.BytesIO fails to change blanks to "#N/A"
    s3_client = boto3.resource("s3")
    s3_object = s3_client.Object(s3_bucket, s3_path)
    obj = s3_object.get()["Body"].read()
    raw_panel = pl.read_excel(
        io.BytesIO(obj),
        # Ignore infer datatype to float as it is not supported by xlsx2csv
        xlsx2csv_options={"ignore_formats": "float"},
        read_csv_options={"infer_schema_length": None},
    )
    return raw_panel.lazy()


@task
def load_fct_panel(s3_bucket: str, s3_path: str):
    s3_client = boto3.resource("s3")
    s3_object = s3_client.Object(s3_bucket, s3_path)
    obj = s3_object.get()["Body"].read()
    fct_panel = pl.read_parquet(io.BytesIO(obj))
    return fct_panel


@task
def select_rows(df: pl.LazyFrame, filters: Mapping[str, List[str]]):
    for col, values in filters.items():
        filtered_df = df.filter(~pl.col(col).is_in(values))
    return filtered_df.collect(streaming=True).lazy()


@task
def clean_raw_panel(
    df: pl.LazyFrame,
    time_col: str,
    target_col: str,
    entity_cols: List[str],
    freq: str,
) -> pl.LazyFrame:

    # Infer datetime format by using the first value
    fmt = infer_dt_format(str(df.collect().select([time_col])[0, 0]))
    new_col_names = {time_col: "time"}
    df_new = (
        df.rename(new_col_names)
        .with_columns(
            [
                # Parse datatime
                (pl.col("time").cast(pl.Utf8).str.strptime(pl.Date, fmt=fmt)),
                # Cast all float cols to int
                pl.col(pl.Float32).cast(pl.Int32),
                pl.col(pl.Float64).cast(pl.Int64),
                # Defensive replace #N/A generated by nulls/blanks to 0
                pl.col(entity_cols).str.replace("#N/A", "0"),
            ]
        )
        # Downcast numeric dtypes
        .select(pl.all().shrink_dtype())
        # Defensive reorder columns
        .select([*entity_cols, "time", target_col])
        # Sort by entity and time
        .sort(by=[*entity_cols, "time"])
        .collect(streaming=True)
    )
    return df_new.lazy()


@task
def add_holiday_effects(df: pl.LazyFrame) -> pl.LazyFrame:
    exprs = [
        pl.col("time").dt.month().is_in([1, 2]).alias("is_holiday"),
    ]
    df_new = df.with_columns(exprs).collect(streaming=True)
    return df_new.lazy()


@task
def add_entity_effects(df: pl.LazyFrame, entity_cols: List[str]) -> pl.LazyFrame:
    fixed_effects = pl.get_dummies(df.collect().select(entity_cols)).select(
        pl.all().cast(pl.Boolean).prefix("is_")
    )
    # Replace spaces and semicolons with underscores
    fixed_effects.columns = [
        re.sub(r"[\s:]", "_", str(col).lower()) for col in fixed_effects.columns
    ]
    df_new = pl.concat([df.collect(), fixed_effects], how="horizontal")
    return df_new.lazy()


@task
def export_fct_panel(
    df: pl.LazyFrame, s3_bucket: str, raw_data_path: str, suffix: Optional[str] = None
):
    # Use hash of raw data path as ID
    identifer = md5(raw_data_path.encode("utf-8")).hexdigest()
    ts = int(datetime.now().timestamp())
    s3_path = f"processed/{identifer}/{ts}"
    if suffix:
        s3_path = f"{s3_path}_{suffix}"
    df.collect().to_pandas().to_parquet(
        f"s3://{s3_bucket}/{s3_path}.parquet", index=False
    )
    return f"{s3_path}.parquet"


@flow
def preprocess_panel(
    s3_bucket: str,
    time_col: str,
    target_col: str,
    entity_cols: List[str],
    freq: str,
    raw_data_path: str,
    manual_forecast_path: Optional[str] = None,
    filters: Optional[Mapping[str, List[str]]] = None,
):

    raw_panel = load_raw_panel(s3_bucket, raw_data_path)
    raw_panel = select_rows(raw_panel, filters) if filters is not None else raw_panel
    fct_panel = clean_raw_panel(
        df=raw_panel,
        time_col=time_col,
        target_col=target_col,
        entity_cols=entity_cols,
        freq=freq,
    )
    paths = {"actual": export_fct_panel(fct_panel, s3_bucket, raw_data_path)}

    if manual_forecast_path is not None:
        manual_forecasts = load_raw_panel(s3_bucket, manual_forecast_path)
        manual_forecasts = (
            select_rows(manual_forecasts, filters)
            if filters is not None
            else manual_forecasts
        )
        fct_manual_forecast = clean_raw_panel(
            df=manual_forecasts,
            time_col=time_col,
            target_col=target_col,
            entity_cols=entity_cols,
            freq=freq,
        )
        paths["manual"] = export_fct_panel(
            fct_manual_forecast, s3_bucket, raw_data_path, suffix="manual"
        )

    return paths


@task
def reindex_ftr_panel(df: pl.DataFrame, suffix: str) -> pl.LazyFrame:
    # Create new index
    entity_col, time_col, target_col = df.columns
    dtypes = df.dtypes[:2]
    entities = df.get_column(entity_col).unique()
    timestamps = df.get_column(time_col).unique()
    full_idx = pl.DataFrame(
        itertools.product(entities, timestamps), columns=df.columns[:2]
    )
    # Defensive cast dtypes to be consistent with df
    full_idx = full_idx.select(
        [pl.col(col).cast(dtypes[i]) for i, col in enumerate(full_idx.columns)]
    )
    # Outer join
    df_new = (
        df.join(full_idx, on=[entity_col, time_col], how="outer")
        # Fill nulls with 0 and add suffix to target col (actual/manual/forecast)
        .with_column(
            pl.col(target_col).fill_null(pl.lit(0)).alias(f"{target_col}:{suffix}")
        ).drop(target_col)
    )
    return df_new.lazy()


@task
def groupby_aggregate(
    df: pl.DataFrame,
    entity_group: List[str],
    target_col: str,
    agg_by: str,
    freq: str,
) -> pl.DataFrame:
    entity_id = ":".join(entity_group)
    agg_methods = {
        "sum": pl.sum(target_col),
        "mean": pl.mean(target_col),
    }

    df_new = (
        # Assign new col with entity_id
        df.with_column(pl.concat_str(entity_group, sep=":").alias(entity_id))
        .sort("time")
        .groupby(["time", entity_id], maintain_order=True)
        .agg(agg_methods[agg_by])
        # Defensive reorder columns
        .select([entity_id, "time", target_col])
        # Defensive resampling
        .groupby_dynamic("time", every=freq, by=entity_id)
        .agg(agg_methods[agg_by])
    )
    return df_new


@task
def filter_negative_values(
    df: pl.DataFrame,
    target_col: str,
) -> pl.DataFrame:

    df_new = df.with_column(
        # Remove negative values from target col
        pl.when(pl.col(target_col) <= 0)
        .then(0)
        .otherwise(pl.col(target_col))
        .keep_name()
    )
    return df_new


@task
def export_ftr_panel(
    df: pl.LazyFrame, s3_bucket: str, fct_data_path: str, suffix: Optional[str] = None
):
    # Use hash of fct data path and entity col as ID
    dataset_id = fct_data_path + df.columns[0]
    identifer = md5(dataset_id.encode("utf-8")).hexdigest()
    ts = int(datetime.now().timestamp())
    s3_path = f"processed/{identifer}/{ts}"
    if suffix:
        s3_path = f"{s3_path}_{suffix}"
    df.collect().to_pandas().to_parquet(
        f"s3://{s3_bucket}/{s3_path}.parquet", index=False
    )
    return f"{s3_path}.parquet"


@flow
def prepare_hierarchical_panel(
    s3_bucket: str,
    entity_group: List[str],
    agg_method: Literal["sum", "mean"],
    fct_panel_path: str,
    target_col: str,
    freq: str,
    manual_forecasts_path: Optional[str] = None,
    allow_negatives: Optional[bool] = False,
):
    merged_entity_col = ":".join(entity_group)
    fct_panel = load_fct_panel(s3_bucket, fct_panel_path)
    ftr_panel = (
        filter_negative_values(fct_panel, target_col)
        if allow_negatives is False
        else fct_panel
    )
    ftr_panel = (
        groupby_aggregate(ftr_panel, entity_group, target_col, agg_method, freq)
        .pipe(reindex_ftr_panel, suffix="actual")
        .pipe(add_holiday_effects)
        .pipe(add_entity_effects, [merged_entity_col])
    )
    paths = {"actual": export_ftr_panel(ftr_panel, s3_bucket, fct_panel_path)}

    if manual_forecasts_path is not None:
        fct_manual_forecast = load_fct_panel(s3_bucket, manual_forecasts_path)
        ftr_manual_forecast = (
            filter_negative_values(fct_manual_forecast, target_col)
            if allow_negatives is False
            else fct_manual_forecast
        )
        ftr_manual_forecast = groupby_aggregate(
            ftr_manual_forecast, entity_group, target_col, agg_method, freq
        ).pipe(reindex_ftr_panel, suffix="manual")
        paths["manual"] = export_ftr_panel(
            ftr_manual_forecast, s3_bucket, fct_panel_path, suffix="manual"
        )

    return paths
